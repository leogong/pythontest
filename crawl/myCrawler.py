#!/usr/bin/python# -*- coding: utf-8 -*-import cookielibimport randomimport tracebackimport urllibfrom lxml.html import soupparserfrom lxml import etreeimport reimport timeimport sysimport MySQLdbimport MySQLdb.cursorsimport chardetimport mechanizeimport qiniufrom qiniu.services.storage import uploader# 1         2       3       4          5        6# localhost dbUser password dbName   domain     urlconn = MySQLdb.connect(host=sys.argv[1], user=sys.argv[2], passwd=sys.argv[3], db=sys.argv[4], port=3306,                       charset='utf8', cursorclass=MySQLdb.cursors.DictCursor)conn2 = MySQLdb.connect(host=sys.argv[1], user='site', passwd=sys.argv[3], db='site', port=3306,                        charset='utf8', cursorclass=MySQLdb.cursors.DictCursor)cur = conn.cursor()cur2 = conn2.cursor()cur2.execute("select * from site where site = %s", (sys.argv[5],))site_info = cur2.fetchone()if site_info is None:    raise Exception("site_info not exists,%s" % sys.argv[5])site_url = site_info['site']br = mechanize.Browser()cj = cookielib.LWPCookieJar()br.set_cookiejar(cj)br.set_handle_equiv(True)br.set_handle_gzip(True)br.set_handle_redirect(False)br.set_handle_referer(True)br.set_handle_robots(False)br.set_handle_refresh(False)br.set_debug_redirects(True)# br.set_handle_refresh(mechanize._http.HTTPRefreshProcessor(), max_time=4)br.addheaders = [('User-agent',                  'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) '                  'Chrome/39.0.2171.65 Safari/537.36')]img_prefix = "wp-content/uploads/%s" % (time.strftime('%Y/%m/%d/%H/', time.localtime(time.time())))my_domain = "http://%s/%s" % (site_info['static_domain'], img_prefix)url_set = set()class Recursion(Exception):    passclass Circle(Exception):    passclass ContentError(Exception):    passclass Article():    title = None    page = None    categories = {}    tags = {}    site_imitation = None    old_url = None    url = None    new_url = None    def __init__(self):        passclass MyCrawler():    url = None    count = 0    final_html = ""    root = None    article = None    site_imitation = None    def __init__(self, url, count=0):        self.url = url        self.count = count        self.article = Article()        self.check()    @staticmethod    def get_site_imitation():        cur2.execute("select * from site_imitation where site_id = %s",                     (site_info['id'],))        all_imitation = cur2.fetchall()        return all_imitation    def check(self):        all_imitation = self.get_site_imitation()        for imitation in all_imitation:            if imitation["domain"] and imitation["domain"] in self.url:                self.site_imitation = imitation        if self.site_imitation is None:            raise Recursion("site not exists," + self.url)        if self.url in url_set:            raise Circle(self.url + " ,circle url")        if len(url_set) > 100:            raise Circle(self.url + " ,too many url")        if self.site_imitation["postfix"] and not self.url.endswith(self.site_imitation["postfix"]):            raise Recursion(self.url + " ,postfix not valid")        url_set.add(self.url)        self.print_info(self.url)    def get_info_from_url(self):        read = br.open(self.url).read()        encoding_ = chardet.detect(read)["encoding"]        if not encoding_:            raise Recursion("encoding is None")        self.root = soupparser.fromstring(read.decode(encoding_))        self.article.old_url = self.url    def replace_pre_code(self):        p = re.compile(r'<pre.*?brush:(.*?);.*?>([\s\S]*?)</pre>')        def func(m):            return '[' + m.group(1).strip() + ']' + m.group(2) + '[/' + m.group(1).strip() + ']'        self.final_html = p.sub(func, self.final_html)        self.final_html = self.final_html.replace("&amp;", "&").replace("&gt;", ">").replace("&lt;", "<").replace(            "&quot;", "\"")    def filter_content(self, html_code):        if self.site_imitation['site_continue']:            continues_ = self.site_imitation['site_continue'].encode("utf-8").split(",")            for keyword in continues_:                if keyword in html_code:                    return True    def filter_ending(self, html_code):        if self.site_imitation['site_break']:            site_breaks = self.site_imitation['site_break'].encode("utf-8").split(",")            for keyword in site_breaks:                if keyword in html_code:                    return True    def save_image(self, img_src):        index = img_src.find("?")        if index > 0:            image_name = img_src[:index]            image_name = image_name[image_name.rindex("/") + 1:]        else:            image_name = img_src[img_src.rindex("/") + 1:]        new_image_url = my_domain + image_name        if not ImageHandler.uploader(img_src, img_prefix + image_name):            raise Recursion("image save failed!," + self.url)        return new_image_url    def replace_img(self, html_code):        img_list = html_code.findall('.//img')        for img_tag in img_list:            img_src = img_tag.get("src")            img_tag.set('src', self.save_image(img_src))    def is_adaptive(self, url):        if url:            all_imitation = self.get_site_imitation()            for imitation in all_imitation:                if imitation['domain'] in url:                    return True    def replace_url(self, html_code):        a_tag_list = html_code.findall('.//a')        for a_tag in a_tag_list:            no_image_child = False            url_href = a_tag.get('href')            if url_href.find("weibo.com") > 0 or url_href.find("t.qq.com") > 0:                my_site_keywords = site_info["keywords"].encode("utf-8").split(",")                a_tag.text = my_site_keywords[random.randint(0, len(my_site_keywords) - 1)].decode('utf-8')            if a_tag is not None:                child_img = a_tag.find('img')                if child_img is not None:                    img_src = child_img.get('src')                    if img_src[img_src.rindex("."):] == url_href[url_href.rindex("."):]:                        a_tag.set('href', img_src)                        no_image_child = True            if not no_image_child and url_href[url_href.rindex(".") + 1:] in (                    'jpg', 'jpeg', 'png', 'bmp', 'JPG', 'JPEG', 'PNG', 'BMP'):                a_tag.set('href', self.save_image(url_href))                continue            if not no_image_child:                if self.is_adaptive(url_href):                    try:                        inner_craw = MyCrawler(url_href, self.count + 1)                        inner_article = inner_craw.start()                        a_tag.set('href', DbHandler.save_article(inner_article))                    except (mechanize.HTTPError, mechanize.URLError) as http_error:                        a_tag.set('href', "http://" + site_url)                        if isinstance(http_error, mechanize.HTTPError):                            self.print_info("%s, http error:%s" % (url_href, http_error.code), self.count + 1)                        else:                            self.print_info("%s, else http error:%s" % (url_href, http_error.reason.args),                                            self.count + 1)                    except Circle, e:                        a_tag.set('href', "http://" + site_url)                        self.print_info(str(e), self.count + 1)                    except ContentError, e:                        a_tag.set('href', "http://" + site_url)                        self.print_info(str(e), self.count + 1)                    except Exception, e:                        a_tag.set('href', "http://" + site_url)                        self.print_info(str(e), self.count + 1)                else:                    # try:                    # br.open(url_href)                    # except (mechanize.HTTPError, mechanize.URLError) as e:                    # if isinstance(e, mechanize.HTTPError):                    # a_tag.set('href', "http://" + site_url)                    # print "url:%s,http error:%s" % (url_href, e.code)                    # else:                    # print "url:%s,else http error:%s" % (url_href, e.reason.args)                    pass    def get_article_content(self):        sub_element = self.root.xpath(self.site_imitation['content'])        if len(sub_element) == 1:            article_sub_element = self.root.find(self.site_imitation['content'])        else:            raise ContentError("no or duplicate content", "url:%s,size:%s" % (self.url, len(sub_element)))        i = 0        for element in article_sub_element:            piece_of_html = etree.tostring(element, encoding="utf-8", method="html")            if self.filter_content(piece_of_html):                continue            if self.filter_ending(piece_of_html):                break            self.replace_img(element)            self.replace_url(element)            self.final_html += etree.tostring(element, encoding="utf-8", method="html")            if i == 2:                self.final_html += "<!--more-->"            self.replace_pre_code()            i += 1        self.article.page = self.final_html        if not self.article.page:            raise Recursion("self.article.page is None i:%s" % i)    def get_categories(self):        if self.site_imitation['categories']:            categories = self.root.xpath(self.site_imitation['categories'])            for category in categories:                r_index__strip = self.get_value_from_url(category.get('href'))                self.article.categories[r_index__strip] = category.text.encode("utf-8").strip()    def get_tags(self):        if self.site_imitation['tags']:            tags = self.root.xpath(self.site_imitation['tags'])            for tag in tags:                r_index__strip = self.get_value_from_url(tag.get('href'))                self.article.tags[r_index__strip] = tag.text.encode("utf-8").strip()    @staticmethod    def get_value_from_url(url):        if url.endswith("/"):            url = url[:url.rindex("/")]        value = url[url.rindex("/") + 1:]        if not value:            value = url[url.rindex("/", 0, len(url) - 1) + 1:]        return value.strip()    def start(self):        cur2.execute("select new_url from site_relation where old_url = %s and "                     "site_id = %s and site_imitation_id=%s",                     (self.url, site_info['id'], self.site_imitation['id']))        exist_page = cur2.fetchone()        if exist_page is not None:            self.print_info(self.url + ", repeat url")            self.article.new_url = exist_page['new_url']            return self.article        self.get_info_from_url()        self.get_article_content()        # todo get keyword description        self.get_categories()        self.get_tags()        self.get_title()        self.replace_keywords()        self.article.site_imitation = self.site_imitation        return self.article    def get_title(self):        self.article.title = self.root.xpath(self.site_imitation["title"])[0].text.encode("utf-8").strip()    def print_info(self, info, count=0):        print "|    " * (self.count if count == 0 else count) + info    def replace_keywords(self):        keywords_ = self.site_imitation["keywords"].encode("utf-8").split(",")        my_site_keywords = site_info["keywords"].encode("utf-8").split(",")        for keyword in keywords_:            self.article.page = self.article.page.replace(keyword, my_site_keywords[                random.randint(0, len(my_site_keywords) - 1)])class DbHandler():    def __init__(self):        pass    @staticmethod    def save_article(article):        if article:            if article.new_url:                return article.new_url            try:                cur.execute("select ID from wp_users where user_login = %s", (site_info['user'],))                user = cur.fetchone()                if user is None:                    raise Recursion("user not exists,user:%s" % site_info['user'])                user_id = user['ID']                cur.execute(                    "INSERT INTO wp_posts(post_author, post_date, post_date_gmt, post_content, "                    "post_title, post_excerpt, post_status,comment_status, ping_status, "                    "post_password, post_name, to_ping, pinged, post_modified, post_modified_gmt, "                    "post_content_filtered, post_parent, guid, menu_order, post_type, post_mime_type, comment_count) "                    "VALUES (%s, now(), now(), %s, %s, '', 'publish', 'open', 'open', '', "                    "%s, '', '', now(), now(), '', 0, '', 0, 'post', '', 0)",                    (user_id, article.page, article.title, urllib.quote(article.title)))                article_id = cur.lastrowid                cur.execute('update wp_posts set guid = %s where ID = %s',                            ("http://" + site_url + "/?p=" + str(article_id), article_id))                for slug, description in article.tags.items():                    DbHandler.save_terms(article_id, slug, description, True)                for slug, description in article.categories.items():                    DbHandler.save_terms(article_id, slug, description)                new_url = "http://%s/%s.html" % (site_url, article_id)                DbHandler.save_relation(new_url, article.site_imitation['id'], article.old_url)                conn.commit()                conn2.commit()                return new_url            except Exception, e:                conn.rollback()                conn2.rollback()                raise Recursion(e)    @staticmethod    def save_terms(article_id, slug, description, is_tag=False):        description = urllib.unquote(description)        taxonomy = "post_tag" if is_tag else "category"        cur.execute(            "select wtt.term_taxonomy_id,wtt.taxonomy,wt.term_id from wp_terms wt inner join wp_term_taxonomy wtt "            "on wt.term_id = wtt.term_id where wt.slug = %s", (slug,))        wp_terms = cur.fetchall()        term_exists = False        exist_wp_term = None        term_id = 0        for wp_term in wp_terms:            term_id = wp_term['term_id']            if wp_term['taxonomy'] == taxonomy:                exist_wp_term = wp_term                term_exists = True                break        if term_exists:            term_taxonomy_id = exist_wp_term['term_taxonomy_id']            cur.execute("update wp_term_taxonomy set count = count+1 where term_taxonomy_id = %s", (term_taxonomy_id,))        else:            cur.execute("INSERT ignore INTO wp_terms(name,slug,term_group) VALUES (%s,%s,%s)",                        (description, slug, 0))            cur.execute(                "insert into wp_term_taxonomy(term_id,taxonomy,description,parent,count) values (%s,%s,%s,%s,%s)",                (term_id if cur.lastrowid == 0 else cur.lastrowid, taxonomy, description, 0, 1))            term_taxonomy_id = cur.lastrowid        cur.execute("insert into wp_term_relationships values (%s,%s,%s)", (article_id, term_taxonomy_id, 0))    @staticmethod    def save_relation(new_url, site_imitation_id, old_url):        cur2.execute("insert into site_relation(site_id, site_imitation_id, old_url, new_url) values (%s,%s,%s,%s)",                     (site_info['id'], site_imitation_id, old_url, new_url))class ImageHandler():    def __init__(self):        pass    @staticmethod    def uploader(url, key):        q = qiniu.Auth(site_info['ak'].encode("utf-8"), site_info['sk'].encode("utf-8"))        data = br.open(url).read()        token = q.upload_token(site_info["bucket_name"].encode("utf-8"))        ret, info = uploader.put_data(token, key, data)        # todo if file exists        # random.sample('zyxwvutsrqponmlkjihgfedcba0123456789',5)        if ret is not None:            return True        else:            print("error msg:" + info + "," + url)  # error message in info            return Falsetry:    # my_craw = MyCrawler("http://ifeve.com/concurrency-optimization-reduce-lock/")    my_craw = MyCrawler(sys.argv[6])    article = my_craw.start()    DbHandler.save_article(article)    conn.commit()    conn2.commit()except Exception, e:    conn.rollback()    conn2.rollback()    print e    print traceback.format_exc()