#!/usr/bin/python# -*- coding: utf-8 -*-import cookielibimport randomimport reimport sysimport timeimport tracebackfrom datetime import datetime, dateimport MySQLdbimport MySQLdb.cursorsimport chardetimport mechanizeimport qiniufrom lxml import etreefrom lxml.html import soupparserfrom qiniu.services.storage import uploaderreload(sys)sys.setdefaultencoding('utf8')# 1         2       3       4          5        6# localhost dbUser password dbName   domain     urlconn = MySQLdb.connect(host=sys.argv[1], user=sys.argv[2], passwd=sys.argv[3], db=sys.argv[4], port=3306,                       charset='utf8', cursorclass=MySQLdb.cursors.DictCursor)conn.autocommit(False)cur = conn.cursor()cur.execute("select * from site where site = %s", (sys.argv[5],))site_info = cur.fetchone()if site_info is None:    raise Exception("site_info not exists,%s" % sys.argv[5])site_url = site_info['site']br = mechanize.Browser()cj = cookielib.LWPCookieJar()br.set_cookiejar(cj)br.set_handle_equiv(True)br.set_handle_gzip(True)br.set_handle_redirect(mechanize.HTTPRedirectHandler)br.set_handle_referer(True)br.set_handle_robots(False)br.set_handle_refresh(False)br.set_debug_redirects(True)# br.set_handle_refresh(mechanize._http.HTTPRefreshProcessor(), max_time=4)br.addheaders = [('User-agent',                  'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) '                  'Chrome/39.0.2171.65 Safari/537.36')]img_prefix = "wp-content/uploads/%s" % (time.strftime('%Y/%m/%d/%H/', time.localtime(time.time())))my_domain = "http://%s/%s" % (site_info['static_domain'], img_prefix)url_set = set()more_ = "<!--more-->"class Recursion(Exception):    passclass Circle(Exception):    passclass ContentError(Exception):    passclass Article:    title = None    page = None    categories = []    tags = []    site_imitation = None    old_url = None    url = None    new_url = None    abstract = None    def __init__(self):        passclass MyCrawler:    url = None    count = 0    final_html = ""    root = None    article = None    site_imitation = None    def __init__(self, url, count=0):        self.url = url[:url.find("#")] if url.find("#") > 0 else url        self.count = count        self.article = Article()        self.check()    @staticmethod    def get_site_imitation():        cur.execute("select * from site_imitation where site_id = %s",                    (site_info['id'],))        all_imitation = cur.fetchall()        return all_imitation    def check(self):        all_imitation = self.get_site_imitation()        for imitation in all_imitation:            imitation_domain_ = imitation["domain"]            if imitation_domain_ and imitation_domain_ in self.url:                self.site_imitation = imitation                break        if self.site_imitation is None:            raise Recursion("site not exists,site:%s" % self.url)        if self.url in url_set:            raise Circle(self.url + " ,circle url")        if len(url_set) > 100:            raise Circle(self.url + " ,too many url")        if self.site_imitation["postfix"] and not self.url.endswith(self.site_imitation["postfix"]):            raise Recursion(self.url + " ,postfix not valid")        url_set.add(self.url)        self.print_info(self.url)    def get_info_from_url(self):        br_open = br.open(self.url)        read = br_open.read()        encoding_ = chardet.detect(read)["encoding"]        if not encoding_:            raise Recursion("encoding is None")        self.root = soupparser.fromstring(read.decode(encoding_))        self.article.old_url = self.url    def replace_pre_code(self):        p = re.compile(r'<pre.*?brush:(.*?);.*?>([\s\S]*?)</pre>')        def func(m):            return '[' + m.group(1).strip() + ']' + m.group(2) + '[/' + m.group(1).strip() + ']'        self.final_html = p.sub(func, self.final_html)        self.final_html = self.final_html.replace("&amp;", "&").replace("&gt;", ">").replace("&lt;", "<").replace(            "&quot;", "\"")    def filter_content(self, html_code):        if self.site_imitation['site_continue']:            continues_ = self.site_imitation['site_continue'].encode("utf-8").split(",")            for keyword in continues_:                if keyword in html_code:                    return True    def filter_ending(self, html_code):        if self.site_imitation['site_break']:            site_breaks = self.site_imitation['site_break'].encode("utf-8").split(",")            for keyword in site_breaks:                if keyword in html_code:                    return True    def save_image(self, img_src):        index = img_src.find("?")        if index > 0:            image_name = img_src[:index]            image_name = image_name[image_name.rindex("/") + 1:]        else:            image_name = img_src[img_src.rindex("/") + 1:]        new_image_url = my_domain + image_name        if not ImageHandler.uploader(img_src, img_prefix + image_name):            raise Recursion("image save failed!," + self.url)        return new_image_url    def replace_img(self, html_code):        img_list = html_code.findall('.//img')        for img_tag in img_list:            img_src = img_tag.get("src")            img_tag.set('src', self.save_image(img_src))    def is_adaptive(self, url):        if url:            all_imitation = self.get_site_imitation()            for imitation in all_imitation:                if imitation['domain'] in url:                    return True    def replace_url(self, html_code):        a_tag_list = html_code.findall('.//a')        for a_tag in a_tag_list:            url_href = a_tag.get('href')            if url_href is None:                a_tag.set('href', "http://" + site_url)                continue            if url_href.find("weibo.com") > 0 or url_href.find("t.qq.com") > 0:                my_site_keywords = site_info["keywords"].encode("utf-8").split(",")                a_tag.text = my_site_keywords[random.randint(0, len(my_site_keywords) - 1)].decode('utf-8')                a_tag.set('href', site_info['weibo'])                continue            child_img = a_tag.find('img')            if child_img is not None:                img_src = child_img.get('src')                if img_src[img_src.rindex("."):] == url_href[url_href.rindex("."):]:                    a_tag.set('href', img_src)                    continue            if url_href.find(".") > 0 and url_href[url_href.rindex(".") + 1:] in (                    'jpg', 'jpeg', 'png', 'bmp', 'JPG', 'JPEG', 'PNG', 'BMP'):                if self.is_adaptive(url_href):                    a_tag.set('href', self.save_image(url_href))                continue            if self.is_adaptive(url_href):                try:                    inner_craw = MyCrawler(url_href, self.count + 1)                    inner_article = inner_craw.start()                    a_tag.set('href', DbHandler.save_article(inner_article))                except (mechanize.HTTPError, mechanize.URLError) as http_error:                    a_tag.set('href', "http://" + site_url)                    if isinstance(http_error, mechanize.HTTPError):                        self.print_info("%s, http error:%s" % (url_href, http_error.code), self.count + 1)                    else:                        self.print_info("%s, else http error:%s" % (url_href, http_error.reason.args),                                        self.count + 1)                except Circle, exception:                    a_tag.set('href', "http://" + site_url)                    self.print_info(str(exception), self.count + 1)                except ContentError, exception:                    a_tag.set('href', "http://" + site_url)                    self.print_info(str(exception), self.count + 1)                except Exception, exception:                    a_tag.set('href', "http://" + site_url)                    self.print_info(str(exception), self.count + 1)            else:                # try:                # br.open(url_href)                # except (mechanize.HTTPError, mechanize.URLError) as e:                # if isinstance(e, mechanize.HTTPError):                # a_tag.set('href', "http://" + site_url)                # print "url:%s,http error:%s" % (url_href, e.code)                # else:                # print "url:%s,else http error:%s" % (url_href, e.reason.args)                pass    def get_article_content(self):        sub_element = self.root.xpath(self.site_imitation['content'])        if len(sub_element) == 1:            article_sub_element = self.root.find(self.site_imitation['content'])        else:            raise ContentError("no or duplicate content", "url:%s,size:%s" % (self.url, len(sub_element)))        i = 0        for element in article_sub_element:            piece_of_html = etree.tostring(element, encoding="utf-8", method="html")            if self.filter_content(piece_of_html):                continue            if self.filter_ending(piece_of_html):                break            self.replace_img(element)            self.replace_url(element)            # self.convert_code_highlight(element)            self.final_html += etree.tostring(element, encoding="utf-8", method="html")            if i == 2:                self.final_html += more_            # self.replace_pre_code()            i += 1        self.article.page = self.final_html        more_index = self.article.page.find(more_)        if more_index > 0:            self.article.abstract = self.article.page[0:more_index]            self.article.page.replace(more_, "")        else:            self.article.abstract = self.article.page        if not self.article.page:            raise Recursion("self.article.page is None i:%s" % i)    def get_categories(self):        if self.site_imitation['categories']:            categories = self.root.xpath(self.site_imitation['categories'])            for category in categories:                self.article.categories.append(                    category.text.encode("utf-8").replace("/", "|").replace("\\", "|").strip())    def get_tags(self):        if self.site_imitation['tags']:            tags = self.root.xpath(self.site_imitation['tags'])            for tag in tags:                self.article.tags.append(tag.text.encode("utf-8").replace("/", "|").replace("\\", "|").strip())    def start(self):        cur.execute("select new_url from site_relation where old_url = %s and "                    "site_id = %s and site_imitation_id=%s",                    (self.url, site_info['id'], self.site_imitation['id']))        exist_page = cur.fetchone()        if exist_page is not None:            self.print_info(self.url + ", repeat url")            self.article.new_url = exist_page['new_url']            return self.article        self.get_info_from_url()        self.get_article_content()        # todo get keyword description        self.get_categories()        self.get_tags()        self.get_title()        self.replace_keywords()        self.article.site_imitation = self.site_imitation        url_set.remove(self.url)        return self.article    def get_title(self):        self.article.title = self.root.xpath(self.site_imitation["title"])[0].text.encode("utf-8").strip()    def print_info(self, info, count=0):        print "|    " * (self.count if count == 0 else count) + info    def replace_keywords(self):        keywords_ = self.site_imitation["keywords"].encode("utf-8").split(",")        my_site_keywords = site_info["keywords"].encode("utf-8").split(",")        for keyword in keywords_:            self.article.page = self.article.page.replace(keyword, my_site_keywords[                random.randint(0, len(my_site_keywords) - 1)])    @staticmethod    def convert_code_highlight(element):        all_pre_elements = element.findall(".//pre")        for element in all_pre_elements:            element.set("class", "prettyprint")class DbHandler:    def __init__(self):        pass    @staticmethod    def save_article(save_article):        if save_article:            if save_article.new_url:                return save_article.new_url            try:                cur.execute("select userEmail from blog_user where userName = %s", (site_info['user'],))                user = cur.fetchone()                if user is None:                    raise Recursion("user not exists,user:%s" % site_info['user'])                user_mail = user['userEmail']                article_id = get_id_in_current_time_millis()                cur.execute(                    "INSERT INTO blog_article VALUES (%s,%s,%s,%s,%s,0,0,%s,%s,1,1,0,now(),now(),%s,1,1,'','tinyMCE')",                    (article_id, save_article.title, save_article.abstract, ','.join(save_article.tags),                     str(user_mail),                     save_article.page, '/' + str(article_id) + '.html', random.uniform(0, 1)))                for tag in save_article.tags:                    DbHandler.save_tags(article_id, tag)                for category in save_article.categories:                    DbHandler.save_tags(article_id, category)                # conn.rollback()                DbHandler.save_statistic(user_mail)                DbHandler.save_archive_date(article_id)                new_url = "http://%s/%s.html" % (site_url, article_id)                DbHandler.save_relation(new_url, save_article.site_imitation['id'], save_article.old_url)                conn.commit()                return new_url            except Exception, exception:                conn.rollback()                raise Recursion(exception)    @staticmethod    def save_tags(article_id, tag):        cur.execute(            "select * FROM blog_tag WHERE tagTitle = %s", (tag,))        tag_record = cur.fetchone()        if tag_record is None:            tag_id = get_id_in_current_time_millis()            cur.execute(                "insert INTO blog_tag VALUES (%s,1,1,%s)", (tag_id, tag)            )        else:            tag_id = tag_record["oId"]            cur.execute(                "UPDATE blog_tag SET "                "tagPublishedRefCount = tagPublishedRefCount+1,tagReferenceCount = tagReferenceCount+1 WHERE oId = %s",                (tag_id,))        cur.execute("insert INTO blog_tag_article VALUES(%s,%s,%s)",                    (get_id_in_current_time_millis(), article_id, tag_id))    @staticmethod    def save_relation(new_url, site_imitation_id, old_url):        cur.execute("insert into site_relation(site_id, site_imitation_id, old_url, new_url) values (%s,%s,%s,%s)",                    (site_info['id'], site_imitation_id, old_url, new_url))    @staticmethod    def save_statistic(email):        cur.execute(            "UPDATE blog_statistic SET "            "statisticBlogArticleCount = statisticBlogArticleCount+1,"            "statisticPublishedBlogArticleCount=statisticPublishedBlogArticleCount+1 "            "WHERE oId='statistic'")        cur.execute(            "UPDATE blog_user SET "            "userArticleCount = userArticleCount+1,"            "userPublishedArticleCount=userPublishedArticleCount+1 WHERE userEmail = %s",            (email,))    @staticmethod    def date_time_milliseconds(date_time_obj):        return int(time.mktime(date_time_obj.timetuple()) * 1000)    @staticmethod    def save_archive_date(article_id):        # noinspection PyCallByClass        archive_time = DbHandler.date_time_milliseconds(            datetime.strptime(datetime.strftime(date.today(), "%Y-%m"), "%Y-%m"))        cur.execute("SELECT oId FROM blog_archiveDate WHERE archiveTime = %s", (archive_time,))        archive_date_record = cur.fetchone()        if archive_date_record is None:            archive_id = get_id_in_current_time_millis()            cur.execute("insert into blog_archiveDate VALUES (%s,1,1,%s)", (archive_id,                                                                            archive_time,))        else:            archive_id = archive_date_record["oId"]            cur.execute(                "UPDATE blog_archiveDate SET "                "archiveDateArticleCount=archiveDateArticleCount+1,"                "archiveDatePublishedArticleCount=archiveDatePublishedArticleCount+1 "                "WHERE oId = %s ", (archive_id,))        DbHandler.save_archive_date_article(archive_id, article_id)    @staticmethod    def save_archive_date_article(archive_id, article_id):        cur.execute("insert INTO blog_archiveDate_article VALUES (%s,%s,%s)", (get_id_in_current_time_millis(),                                                                               archive_id, article_id))class ImageHandler:    def __init__(self):        pass    @staticmethod    def uploader(url, key):        q = qiniu.Auth(site_info['ak'].encode("utf-8"), site_info['sk'].encode("utf-8"))        data = br.open(url).read()        token = q.upload_token(site_info["bucket_name"].encode("utf-8"))        ret, info = uploader.put_data(token, key, data)        # todo if file exists        # random.sample('zyxwvutsrqponmlkjihgfedcba0123456789',5)        if ret is not None:            return True        else:            print("error msg:" + info + "," + url)  # error message in info            return Falsedef get_id_in_current_time_millis():    time.sleep(0.005)    return long(time.time() * 1000)def list_handle(list_url):    imitation = None    imitations = MyCrawler.get_site_imitation()    for one_of_imitation in imitations:        imitation_domain_ = one_of_imitation["domain"]        if imitation_domain_ and imitation_domain_ in list_url:            imitation = one_of_imitation            break    if imitation is None:        raise Exception("site does not support,url:%s" % list_url)    read = br.open(list_url).read()    encoding_ = chardet.detect(read)["encoding"]    if not encoding_:        raise Recursion("encoding is None")    root = soupparser.fromstring(read.decode(encoding_))    article_urls = root.xpath(imitation["list"])    for article_url in article_urls:        try:            detail_craw = MyCrawler(article_url.get("href"))            article = detail_craw.start()            DbHandler.save_article(article)            conn.commit()        except Exception, e:            conn.rollback()            print e            print traceback.format_exc()def detail_handle(url):    try:        my_craw = MyCrawler(url)        article = my_craw.start()        DbHandler.save_article(article)        conn.commit()    except Exception, e:        conn.rollback()        print e        print traceback.format_exc()CMDS = {'d': detail_handle, 'l': list_handle}def main():    argv_ = sys.argv[6]    equal_flag_index = argv_.find("=")    if equal_flag_index <= 0:        raise Exception("didn't find '='")    flag = argv_[0:equal_flag_index]    url = argv_[equal_flag_index + 1:]    operation = CMDS[flag]    operation(url)if __name__ == '__main__':    main()